#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Excel Unifier - í†µì¼ë˜ì§€ ì•Šì€ ì—‘ì…€ íŒŒì¼ë“¤ì„ ë¶„ì„í•˜ê³  í†µí•©í•˜ëŠ” ë„êµ¬
"""

import pandas as pd
import os
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from fuzzywuzzy import fuzz
from collections import defaultdict
import json
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# KFTA íŒŒì„œ import
try:
    from kfta_parser import KFTAParser
except ImportError:
    KFTAParser = None


class ExcelUnifier:
    def __init__(
        self,
        similarity_threshold: int = 85,
        use_ai: bool = False,
        gemini_api_key: Optional[str] = None
    ):
        """
        ì—‘ì…€ í†µí•©ê¸° ì´ˆê¸°í™”

        Args:
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (0-100, ê¸°ë³¸ê°’ 85)
            use_ai: AI ê¸°ë°˜ ë§¤ì¹­ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’ False)
            gemini_api_key: Gemini API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ìŒ)
        """
        self.similarity_threshold = similarity_threshold
        self.use_ai = use_ai
        self.dataframes = []
        self.column_mappings = {}
        self.unified_columns = []

        # AI ëª¨ë“œ ì´ˆê¸°í™”
        self.ai_matcher = None
        if use_ai:
            try:
                from ai_matcher import GeminiMatcher
                self.ai_matcher = GeminiMatcher(api_key=gemini_api_key)
                print("ğŸ¤– AI ëª¨ë“œ í™œì„±í™” (Gemini API)")
            except ImportError:
                print("âš ï¸  ai_matcher ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                self.use_ai = False
            except Exception as e:
                print(f"âš ï¸  AI ëª¨ë“œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}. ê¸°ë³¸ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                self.use_ai = False

    def load_excel_files(self, file_paths: List[str]) -> None:
        """ì—¬ëŸ¬ ì—‘ì…€ íŒŒì¼ ë¡œë“œ (ëª¨ë“  ì‹œíŠ¸ í¬í•¨)"""
        print(f"ğŸ“‚ {len(file_paths)}ê°œì˜ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤...")

        for file_path in file_paths:
            try:
                # ì—‘ì…€ íŒŒì¼ ì½ê¸° (.xlsx, .xls ëª¨ë‘ ì§€ì›)
                if file_path.endswith('.csv'):
                    df = pd.read_csv(file_path)
                    self.dataframes.append({
                        'path': file_path,
                        'sheet': None,
                        'data': df,
                        'columns': list(df.columns)
                    })
                    print(f"  âœ“ {os.path.basename(file_path)}: {len(df)}í–‰, {len(df.columns)}ê°œ ì»¬ëŸ¼")
                else:
                    # ì—‘ì…€ íŒŒì¼ì˜ ëª¨ë“  ì‹œíŠ¸ ì½ê¸°
                    excel_file = pd.ExcelFile(file_path)
                    sheet_names = excel_file.sheet_names

                    print(f"  ğŸ“„ {os.path.basename(file_path)}: {len(sheet_names)}ê°œ ì‹œíŠ¸ ë°œê²¬")

                    for sheet_name in sheet_names:
                        df = pd.read_excel(file_path, sheet_name=sheet_name)

                        # ë¹ˆ ì‹œíŠ¸ ê±´ë„ˆë›°ê¸°
                        if df.empty or len(df.columns) == 0:
                            print(f"    âŠ˜ ì‹œíŠ¸ '{sheet_name}': ë¹ˆ ì‹œíŠ¸ (ê±´ë„ˆëœ€)")
                            continue

                        self.dataframes.append({
                            'path': file_path,
                            'sheet': sheet_name,
                            'data': df,
                            'columns': list(df.columns)
                        })
                        print(f"    âœ“ ì‹œíŠ¸ '{sheet_name}': {len(df)}í–‰, {len(df.columns)}ê°œ ì»¬ëŸ¼")

            except Exception as e:
                print(f"  âœ— {file_path} ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

    def analyze_columns(self) -> Dict[str, List[str]]:
        """
        ëª¨ë“  íŒŒì¼ì˜ ì»¬ëŸ¼ëª…ì„ ë¶„ì„í•˜ê³  ìœ ì‚¬í•œ ì»¬ëŸ¼ë¼ë¦¬ ê·¸ë£¹í™”

        Returns:
            ì»¬ëŸ¼ ê·¸ë£¹ ë”•ì…”ë„ˆë¦¬ {í†µí•©ì»¬ëŸ¼ëª…: [ì›ë³¸ì»¬ëŸ¼ëª…ë“¤]}
        """
        print("\nğŸ” ì»¬ëŸ¼ëª… ë¶„ì„ ì¤‘...")

        # í‚¤ì›Œë“œ ê¸°ë°˜ ì»¬ëŸ¼ ë§¤í•‘ ê·œì¹™
        # ìˆœì„œ ì¤‘ìš”: ë” êµ¬ì²´ì ì¸ ê²ƒì„ ë¨¼ì € ë°°ì¹˜
        keyword_mappings = {
            # ê°•ì›êµì´ í‘œì¤€ ë§¤í•‘ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
            'í˜„ì¬êµìœ¡ì²­': ['í˜„ì¬êµìœ¡ì²­', 'í˜„ êµìœ¡ì²­', 'ì†Œì†êµìœ¡ì²­', 'ì›êµìœ¡ì²­', 'í˜„ì¬ êµìœ¡ì²­'],
            'í˜„ì¬ë³¸ì²­': ['í˜„ì¬ë³¸ì²­', 'í˜„ì¬ ë³¸ì²­', 'í˜„ì¬í•™êµ', 'í˜„ ë³¸ì²­', 'í˜„ì¬ í•™êµ', 'ì†Œì†í•™êµ', 'ì†Œì†ë³¸ì²­', 'í˜„ì¬ë¶„íšŒ', 'í˜„ì¬ ë¶„íšŒ'],
            'ëŒ€ì‘': ['ëŒ€ì‘', 'ëŒ€ ì‘', 'ì´ë¦„', 'ì„±ëª…'],
            'ë°œë ¹êµìœ¡ì²­': ['ë°œë ¹êµìœ¡ì²­', 'ë°œë ¹ êµìœ¡ì²­', 'ì „ì…êµìœ¡ì²­', 'ë°°ì¹˜êµìœ¡ì²­', 'ë°œë ¹êµ ìœ¡ì²­'],
            'ë°œë ¹ë³¸ì²­': ['ë°œë ¹ë³¸ì²­', 'ë°œë ¹ ë³¸ì²­', 'ì „ì…í•™êµ', 'ë°°ì¹˜í•™êµ', 'ë°œë ¹í•™êµ', 'ë°œë ¹ í•™êµ', 'ì „ë³´í•™êµ', 'ë°œë ¹ë¶„íšŒ', 'ë°œë ¹ ë¶„íšŒ'],
            'ê³¼ëª©': ['ê³¼ëª©', 'êµê³¼', 'ë‹´ë‹¹ê³¼ëª©', 'ê³¼ ëª©', 'êµ ê³¼'],
            'ì§ìœ„': ['ì§ìœ„', 'ì§ ìœ„', 'ë³´ì§', 'ì§ê¸‰'],
            'ì§ì¢…ë¶„ë¥˜': ['ì§ì¢…ë¶„ë¥˜', 'ì§ì¢… ë¶„ë¥˜', 'ì§ì¢…', 'êµì›êµ¬ë¶„', 'êµì‚¬êµ¬ë¶„', 'êµì§ì›êµ¬ë¶„'],
            'ë¶„ë¥˜ëª…': ['ë¶„ë¥˜ëª…', 'ë¶„ë¥˜ ëª…', 'ë¶„ë¥˜'],
            'ì·¨ê¸‰ì½”ë“œ': ['ì·¨ê¸‰ì½”ë“œ', 'ì·¨ê¸‰ ì½”ë“œ', 'êµ¬ë¶„ì½”ë“œ'],
            'ì‹œêµ°êµ¬ë¶„': ['ì‹œêµ°êµ¬ë¶„', 'ì‹œêµ° êµ¬ë¶„', 'ì‹œêµ°', 'ì§€ì—­êµ¬ë¶„'],
            'êµí˜¸ê¸°í˜¸ë“±': ['êµí˜¸ê¸°í˜¸ë“±', 'êµí˜¸ ê¸°í˜¸ë“±', 'êµí˜¸ê¸°í˜¸', 'ê³ í˜¸ê¸°í˜¸ë“±'],

            # ì¼ë°˜ ë§¤í•‘ (ìš°ì„ ìˆœìœ„ ë‚®ìŒ)
            'ì´ë¦„': ['ì´ë¦„', 'ì„±ëª…', 'í•™ìƒëª…', 'ì„±í•¨', 'ì´ ë¦„', 'êµì‚¬ëª…', 'êµì›ëª…'],
            'í•™êµ': ['í•™êµ', 'ëŒ€í•™êµ', 'ì†Œì†ëŒ€í•™', 'ëŒ€í•™', 'í•™ êµ', 'ë³¸ì²­'],
            'ì „ê³µ': ['ì „ê³µ', 'ì „ê³µë¶„ì•¼', 'ì „ê³µê³¼ëª©', 'í•™ê³¼', 'ì „ ê³µ'],
            'í•™ë…„': ['í•™ë…„', 'í•™ ë…„', 'ì—°ì°¨'],
            'ì—°ë½ì²˜': ['ì—°ë½ì²˜', 'ì „í™”ë²ˆí˜¸', 'íœ´ëŒ€í°', 'ì „í™”', 'ì—° ë½ ì²˜', 'í•¸ë“œí°', 'HP', 'íœ´ëŒ€ì „í™”'],
            'ì´ë©”ì¼': ['ì´ë©”ì¼', 'ë©”ì¼', 'email', 'e-mail', 'ì´ ë©” ì¼'],
            'ì£¼ì†Œ': ['ì£¼ì†Œ', 'ì£¼ ì†Œ', 'ê±°ì£¼ì§€', 'ì§‘ì£¼ì†Œ'],
        }

        # ëª¨ë“  ì»¬ëŸ¼ëª… ìˆ˜ì§‘
        all_columns = []
        for df_info in self.dataframes:
            all_columns.extend(df_info['columns'])

        # ì¤‘ë³µ ì œê±°í•˜ë˜ ë¹ˆë„ìˆ˜ ìœ ì§€
        column_freq = defaultdict(int)
        for col in all_columns:
            column_freq[col] += 1

        unique_columns = list(column_freq.keys())

        # ë°œê²¬ëœ ëª¨ë“  ì»¬ëŸ¼ ì¶œë ¥
        print(f"  ğŸ“‹ ë°œê²¬ëœ ì»¬ëŸ¼: {unique_columns}")

        # ì»¬ëŸ¼ ê·¸ë£¹í™”
        column_groups = {}
        processed = set()

        # ë¨¼ì € í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤í•‘ ì ìš©
        for unified_name, keywords in keyword_mappings.items():
            matched_cols = []
            for col in unique_columns:
                if col in processed:
                    continue
                col_normalized = col.strip().replace(' ', '').lower()
                for keyword in keywords:
                    keyword_normalized = keyword.strip().replace(' ', '').lower()
                    # ì •í™•íˆ ì¼ì¹˜í•˜ê±°ë‚˜ í¬í•¨í•˜ëŠ” ê²½ìš°
                    if col_normalized == keyword_normalized or keyword_normalized in col_normalized:
                        matched_cols.append(col)
                        processed.add(col)
                        print(f"  âœ“ '{col}' â†’ '{unified_name}' (í‚¤ì›Œë“œ: '{keyword}')")
                        break

            if matched_cols:
                column_groups[unified_name] = matched_cols
                if len(matched_cols) > 1:
                    print(f"  ğŸ“Œ '{unified_name}' â† {matched_cols}")

        # ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ë“¤ì€ ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ë§¤í•‘
        for i, col1 in enumerate(unique_columns):
            if col1 in processed:
                continue

            # í˜„ì¬ ì»¬ëŸ¼ê³¼ ìœ ì‚¬í•œ ì»¬ëŸ¼ë“¤ ì°¾ê¸°
            similar_cols = [col1]
            processed.add(col1)

            for col2 in unique_columns[i+1:]:
                if col2 in processed:
                    continue

                # ìœ ì‚¬ë„ ê³„ì‚° (AI ëª¨ë“œ ë˜ëŠ” ê¸°ë³¸ ëª¨ë“œ)
                is_similar = False

                if self.use_ai and self.ai_matcher:
                    # AI ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
                    try:
                        result = self.ai_matcher.calculate_semantic_similarity(
                            col1, col2,
                            context="ì—‘ì…€ ì»¬ëŸ¼ëª…"
                        )
                        is_similar = result['is_similar']
                        if is_similar:
                            print(f"  ğŸ¤– AI ë§¤ì¹­: '{col1}' â†” '{col2}' ({result['similarity']}%, {result['reason']})")
                    except Exception as e:
                        print(f"  âš ï¸  AI ë¶„ì„ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë“œë¡œ ì „í™˜: {str(e)}")
                        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ëª¨ë“œë¡œ fallback
                        similarity = fuzz.ratio(col1, col2)
                        is_similar = similarity >= self.similarity_threshold
                else:
                    # ê¸°ë³¸ ëª¨ë“œ: Levenshtein Distance
                    similarity = fuzz.ratio(col1, col2)
                    is_similar = similarity >= self.similarity_threshold

                if is_similar:
                    similar_cols.append(col2)
                    processed.add(col2)

            # ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ ì»¬ëŸ¼ëª…ì„ ëŒ€í‘œ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì„ íƒ
            representative = max(similar_cols, key=lambda x: column_freq[x])
            column_groups[representative] = similar_cols

            if len(similar_cols) > 1:
                print(f"  ğŸ“Œ '{representative}' â† {similar_cols}")

        self.column_mappings = column_groups
        self.unified_columns = list(column_groups.keys())

        # ë§¤í•‘ë˜ì§€ ì•Šì€ ì»¬ëŸ¼ í‘œì‹œ
        unmatched = [col for col in unique_columns if col not in processed]
        if unmatched:
            print(f"  âš ï¸  ë§¤í•‘ë˜ì§€ ì•Šì€ ì»¬ëŸ¼ ({len(unmatched)}ê°œ): {unmatched}")

        print(f"\nâœ… ì´ {len(column_groups)}ê°œì˜ í†µí•© ì»¬ëŸ¼ ìƒì„±")

        return column_groups

    def normalize_value(self, value: str, value_type: str = 'general') -> str:
        """
        ê°’ ì •ê·œí™”

        Args:
            value: ì •ê·œí™”í•  ê°’
            value_type: ê°’ì˜ íƒ€ì… (general, school, name ë“±)
        """
        if pd.isna(value):
            return ""

        value = str(value).strip()

        # í•™êµëª… ì •ê·œí™”
        if value_type == 'school':
            # ëŒ€í•™êµ, ëŒ€í•™, ê³ ë“±í•™êµ, ì¤‘í•™êµ ë“± í†µì¼
            replacements = [
                ('å¤§å­¸æ ¡', ''),
                ('å¤§å­¦æ ¡', ''),
                ('å¤§å­¦', ''),
                ('ëŒ€í•™êµ', ''),
                ('ëŒ€í•™', ''),
                ('ê³ ë“±í•™êµ', ''),
                ('ê³ êµ', ''),
                ('ì¤‘í•™êµ', ''),
                ('ì¤‘í•™', ''),
                (' ', ''),
            ]
            for old, new in replacements:
                value = value.replace(old, new)

        return value.lower()

    def find_similar_values(self, values: List[str], threshold: int = None) -> Dict[str, List[str]]:
        """
        ìœ ì‚¬í•œ ê°’ë“¤ì„ ê·¸ë£¹í™”

        Args:
            values: ê°’ ë¦¬ìŠ¤íŠ¸
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’

        Returns:
            {ëŒ€í‘œê°’: [ìœ ì‚¬í•œ ê°’ë“¤]} ë”•ì…”ë„ˆë¦¬
        """
        if threshold is None:
            threshold = self.similarity_threshold

        unique_values = list(set([str(v) for v in values if pd.notna(v) and str(v).strip()]))

        value_groups = {}
        processed = set()

        for i, val1 in enumerate(unique_values):
            if val1 in processed:
                continue

            similar_vals = [val1]
            processed.add(val1)

            for val2 in unique_values[i+1:]:
                if val2 in processed:
                    continue

                similarity = fuzz.ratio(val1, val2)
                if similarity >= threshold:
                    similar_vals.append(val2)
                    processed.add(val2)

            # ê°€ì¥ ê¸´ ê°’ì„ ëŒ€í‘œê°’ìœ¼ë¡œ (ë³´í†µ ë” ì™„ì „í•œ í˜•íƒœ)
            representative = max(similar_vals, key=len)
            value_groups[representative] = similar_vals

        return value_groups

    def unify_dataframes(self, key_columns: List[str] = None, output_format: str = 'auto') -> pd.DataFrame:
        """
        ëª¨ë“  ë°ì´í„°í”„ë ˆì„ì„ í†µí•©

        Args:
            key_columns: ì¤‘ë³µ íŒë‹¨ì— ì‚¬ìš©í•  í‚¤ ì»¬ëŸ¼ë“¤ (ì˜ˆ: ['ì´ë¦„', 'í•™êµ'])
            output_format: ì¶œë ¥ í˜•ì‹ ('auto', 'standard', 'kfta')
                - 'auto': ìë™ ê°ì§€ (ê¸°ë³¸ê°’)
                - 'standard': ëª¨ë“  ì»¬ëŸ¼ í¬í•¨
                - 'kfta': ê°•ì›êµì´ í‘œì¤€ í˜•ì‹ (12ê°œ ì»¬ëŸ¼)

        Returns:
            í†µí•©ëœ ë°ì´í„°í”„ë ˆì„
        """
        print("\nğŸ”„ ë°ì´í„° í†µí•© ì¤‘...")

        if not self.column_mappings:
            self.analyze_columns()

        # í†µí•© ë°ì´í„°í”„ë ˆì„ ì´ˆê¸°í™”
        unified_data = []

        # ê° íŒŒì¼ì˜ ë°ì´í„°ë¥¼ í†µì¼ëœ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë³€í™˜
        for df_info in self.dataframes:
            df = df_info['data'].copy()

            # KFTA í˜•ì‹ì´ê³  íŒŒì„œê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ íŠ¹ìˆ˜ íŒŒì‹± ì ìš©
            if output_format == 'kfta' and KFTAParser is not None:
                parser = KFTAParser(use_ai=self.use_ai, ai_matcher=self.ai_matcher)
                df_unified = parser.parse_dataframe(df)

                file_name = os.path.basename(df_info['path'])
                sheet_info = f" (ì‹œíŠ¸: {df_info['sheet']})" if df_info.get('sheet') else ""
                print(f"  âœ“ {file_name}{sheet_info}: {len(df_unified)}í–‰ ë³€í™˜ (KFTA íŒŒì„œ ì‚¬ìš©)")

                unified_data.append(df_unified)
                continue

            # KFTA í˜•ì‹ì´ì§€ë§Œ íŒŒì„œ ë¯¸ì‚¬ìš© ì‹œ, ë˜ëŠ” ì¼ë°˜ ë§¤í•‘ ë°©ì‹
            # ì»¬ëŸ¼ëª… ë§¤í•‘
            rename_dict = {}
            for unified_col, original_cols in self.column_mappings.items():
                for orig_col in original_cols:
                    if orig_col in df.columns:
                        rename_dict[orig_col] = unified_col

            df_renamed = df.rename(columns=rename_dict)

            # ì¤‘ë³µëœ ì»¬ëŸ¼ëª… ì²˜ë¦¬ (ê°™ì€ ì´ë¦„ì˜ ì»¬ëŸ¼ì´ ì—¬ëŸ¬ ê°œ ìˆëŠ” ê²½ìš°)
            if df_renamed.columns.duplicated().any():
                # ì¤‘ë³µ ì»¬ëŸ¼ ì œê±° (ì²« ë²ˆì§¸ë§Œ ìœ ì§€)
                df_renamed = df_renamed.loc[:, ~df_renamed.columns.duplicated()]

            # ëˆ„ë½ëœ ì»¬ëŸ¼ ì¶”ê°€ (ë¹ˆ ê°’ìœ¼ë¡œ)
            for col in self.unified_columns:
                if col not in df_renamed.columns:
                    df_renamed[col] = ""

            # í†µì¼ëœ ì»¬ëŸ¼ë§Œ ì„ íƒ
            df_unified = df_renamed[self.unified_columns]
            unified_data.append(df_unified)

            # ì‹œíŠ¸ ì •ë³´ í¬í•¨í•˜ì—¬ ì¶œë ¥
            file_name = os.path.basename(df_info['path'])
            sheet_info = f" (ì‹œíŠ¸: {df_info['sheet']})" if df_info.get('sheet') else ""
            print(f"  âœ“ {file_name}{sheet_info}: {len(df_unified)}í–‰ ë³€í™˜")

        # ëª¨ë“  ë°ì´í„° ê²°í•©
        result_df = pd.concat(unified_data, ignore_index=True)
        print(f"\nğŸ“Š í†µí•© ì™„ë£Œ: ì´ {len(result_df)}í–‰")

        # ì¶œë ¥ í˜•ì‹ ì ìš© (KFTA íŒŒì„œë¥¼ ì‚¬ìš©í•œ ê²½ìš° ì´ë¯¸ í‘œì¤€ í˜•ì‹ì´ë¯€ë¡œ ê±´ë„ˆëœ€)
        if output_format == 'kfta' and KFTAParser is None:
            # KFTA íŒŒì„œë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì ìš©
            result_df = self._apply_kfta_format(result_df)
        elif output_format == 'auto':
            # ê°•ì›êµì´ í˜•ì‹ ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            kfta_columns = ['í˜„ì¬êµìœ¡ì²­', 'í˜„ì¬ë³¸ì²­', 'ëŒ€ì‘', 'ë°œë ¹êµìœ¡ì²­', 'ë°œë ¹ë³¸ì²­']
            if any(col in result_df.columns for col in kfta_columns):
                # ì´ë¯¸ KFTA ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ í¬ë§·ë§Œ ì ìš©
                if len(result_df.columns) != 12:  # 12ê°œ í‘œì¤€ ì»¬ëŸ¼ì´ ì•„ë‹ˆë©´
                    result_df = self._apply_kfta_format(result_df)

        # í‚¤ ì»¬ëŸ¼ì´ ì§€ì •ëœ ê²½ìš° ì¤‘ë³µ ì œê±°
        if key_columns:
            print(f"\nğŸ”‘ í‚¤ ì»¬ëŸ¼ {key_columns}ë¡œ ì¤‘ë³µ í™•ì¸ ì¤‘...")

            # í‚¤ ì»¬ëŸ¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            valid_keys = [k for k in key_columns if k in result_df.columns]

            if valid_keys:
                before_count = len(result_df)
                result_df = self._remove_duplicates_smart(result_df, valid_keys)
                after_count = len(result_df)
                removed = before_count - after_count

                if removed > 0:
                    print(f"  âœ“ {removed}ê°œì˜ ì¤‘ë³µ í–‰ ì œê±°ë¨")
                else:
                    print(f"  â„¹ ì¤‘ë³µ í–‰ì´ ë°œê²¬ë˜ì§€ ì•ŠìŒ")

        return result_df

    def _remove_duplicates_smart(self, df: pd.DataFrame, key_columns: List[str]) -> pd.DataFrame:
        """
        ìŠ¤ë§ˆíŠ¸ ì¤‘ë³µ ì œê±° - ìœ ì‚¬í•œ ê°’ë„ ê°™ì€ ê²ƒìœ¼ë¡œ ê°„ì£¼
        """
        # ì •ê·œí™”ëœ í‚¤ ì»¬ëŸ¼ ìƒì„±
        normalized_df = df.copy()

        for col in key_columns:
            if col in df.columns:
                # ê°’ ì •ê·œí™”
                col_type = 'school' if 'í•™êµ' in col else 'general'
                normalized_df[f'{col}_normalized'] = df[col].apply(
                    lambda x: self.normalize_value(x, col_type)
                )

        # ì •ê·œí™”ëœ ê°’ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
        normalized_keys = [f'{col}_normalized' for col in key_columns if col in df.columns]

        if normalized_keys:
            # ì¤‘ë³µ ì¤‘ ì²« ë²ˆì§¸ í–‰ ìœ ì§€ (ê°€ì¥ ì™„ì „í•œ ë°ì´í„°ë¥¼ ê°€ì§„ í–‰ ì„ íƒ)
            result_df = normalized_df.drop_duplicates(subset=normalized_keys, keep='first')

            # ì •ê·œí™” ì»¬ëŸ¼ ì œê±°
            result_df = result_df.drop(columns=normalized_keys)
        else:
            result_df = df

        return result_df

    def _apply_kfta_format(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ê°•ì›êµì´ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜

        í‘œì¤€ ì»¬ëŸ¼ ìˆœì„œ:
        1. í˜„ì¬êµìœ¡ì²­
        2. í˜„ì¬ë³¸ì²­
        3. ëŒ€ì‘
        4. ë°œë ¹êµìœ¡ì²­
        5. ë°œë ¹ë³¸ì²­
        6. ê³¼ëª©
        7. ì§ìœ„
        8. ì§ì¢…ë¶„ë¥˜
        9. ë¶„ë¥˜ëª…
        10. ì·¨ê¸‰ì½”ë“œ
        11. ì‹œêµ°êµ¬ë¶„
        12. êµí˜¸ê¸°í˜¸ë“±
        """
        print("\nğŸ“‹ ê°•ì›êµì´ í‘œì¤€ í˜•ì‹ ì ìš© ì¤‘...")

        # í‘œì¤€ ì»¬ëŸ¼ ìˆœì„œ
        KFTA_COLUMNS = [
            'í˜„ì¬êµìœ¡ì²­',
            'í˜„ì¬ë³¸ì²­',
            'ëŒ€ì‘',
            'ë°œë ¹êµìœ¡ì²­',
            'ë°œë ¹ë³¸ì²­',
            'ê³¼ëª©',
            'ì§ìœ„',
            'ì§ì¢…ë¶„ë¥˜',
            'ë¶„ë¥˜ëª…',
            'ì·¨ê¸‰ì½”ë“œ',
            'ì‹œêµ°êµ¬ë¶„',
            'êµí˜¸ê¸°í˜¸ë“±'
        ]

        result_df = df.copy()

        # ëˆ„ë½ëœ ì»¬ëŸ¼ ì¶”ê°€ (ë¹ˆ ê°’ìœ¼ë¡œ)
        for col in KFTA_COLUMNS:
            if col not in result_df.columns:
                result_df[col] = ""
                print(f"  â„¹ ì»¬ëŸ¼ '{col}' ì¶”ê°€ (ë¹ˆ ê°’)")

        # í‘œì¤€ ìˆœì„œëŒ€ë¡œ ì»¬ëŸ¼ ì¬ì •ë ¬
        result_df = result_df[KFTA_COLUMNS]

        print(f"  âœ“ í‘œì¤€ í˜•ì‹ ì ìš© ì™„ë£Œ: {len(KFTA_COLUMNS)}ê°œ ì»¬ëŸ¼")

        return result_df

    def save_unified_excel(self, output_path: str, df: pd.DataFrame = None) -> None:
        """í†µí•©ëœ ë°ì´í„°ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥ (íƒ€ì‹œë„ í•™êµ ë¹¨ê°„ìƒ‰ í‘œì‹œ)"""
        if df is None:
            df = self.unify_dataframes()

        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘: {output_path}")

        # ì—‘ì…€ë¡œ ì €ì¥
        df.to_excel(output_path, index=False, engine='openpyxl')

        # íƒ€ì‹œë„ í•™êµ ì…€ ìƒ‰ìƒ í‘œì‹œ
        try:
            from openpyxl import load_workbook
            from openpyxl.styles import PatternFill

            wb = load_workbook(output_path)
            ws = wb.active

            # ë¹¨ê°„ìƒ‰ ë°°ê²½ ì„¤ì •
            red_fill = PatternFill(start_color="FFCCCC", end_color="FFCCCC", fill_type="solid")

            # íƒ€ì‹œë„ ì§€ì—­ ë¦¬ìŠ¤íŠ¸
            OTHER_REGIONS = [
                'ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ì¸ì²œ', 'ê´‘ì£¼', 'ëŒ€ì „', 'ìš¸ì‚°', 'ì„¸ì¢…',
                'ê²½ê¸°', 'ì¶©ë¶', 'ì¶©ë‚¨', 'ì „ë¶', 'ì „ë‚¨', 'ê²½ë¶', 'ê²½ë‚¨', 'ì œì£¼'
            ]

            # í˜„ì¬êµìœ¡ì²­ê³¼ ë°œë ¹êµìœ¡ì²­, í˜„ì¬ë³¸ì²­, ë°œë ¹ë³¸ì²­ ì»¬ëŸ¼ ì¸ë±ìŠ¤ ì°¾ê¸°
            columns = list(df.columns)
            col_indices = {}
            for col_name in ['í˜„ì¬êµìœ¡ì²­', 'ë°œë ¹êµìœ¡ì²­', 'í˜„ì¬ë³¸ì²­', 'ë°œë ¹ë³¸ì²­']:
                if col_name in columns:
                    # Excel ì»¬ëŸ¼ì€ 1ë¶€í„° ì‹œì‘ (A=1, B=2, ...)
                    col_indices[col_name] = columns.index(col_name) + 1

            # ê° í–‰ ê²€ì‚¬ (í—¤ë” ì œì™¸, ë°ì´í„°ëŠ” 2í–‰ë¶€í„°)
            colored_count = 0
            for row_idx in range(2, len(df) + 2):  # Excelì€ 1-based, í—¤ë”ê°€ 1í–‰
                is_other_region = False

                # êµìœ¡ì²­ì´ ë¹ˆ ë¬¸ìì—´ì¸ì§€ í™•ì¸
                if 'í˜„ì¬êµìœ¡ì²­' in col_indices or 'ë°œë ¹êµìœ¡ì²­' in col_indices:
                    for col_name in ['í˜„ì¬êµìœ¡ì²­', 'ë°œë ¹êµìœ¡ì²­']:
                        if col_name in col_indices:
                            cell_value = ws.cell(row=row_idx, column=col_indices[col_name]).value
                            # êµìœ¡ì²­ì´ ë¹„ì–´ìˆê³  í•™êµëª…ì´ ìˆìœ¼ë©´ íƒ€ì‹œë„ì¼ ê°€ëŠ¥ì„±
                            if not cell_value or str(cell_value).strip() == '':
                                # í•´ë‹¹ ë³¸ì²­ í™•ì¸
                                ë³¸ì²­_col = 'í˜„ì¬ë³¸ì²­' if col_name == 'í˜„ì¬êµìœ¡ì²­' else 'ë°œë ¹ë³¸ì²­'
                                if ë³¸ì²­_col in col_indices:
                                    school_value = ws.cell(row=row_idx, column=col_indices[ë³¸ì²­_col]).value
                                    if school_value and str(school_value).strip():
                                        is_other_region = True
                                        break

                # í•™êµëª…ì— íƒ€ì‹œë„ ì§€ì—­ëª…ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                if not is_other_region:
                    for col_name in ['í˜„ì¬ë³¸ì²­', 'ë°œë ¹ë³¸ì²­']:
                        if col_name in col_indices:
                            cell_value = ws.cell(row=row_idx, column=col_indices[col_name]).value
                            if cell_value:
                                for region in OTHER_REGIONS:
                                    if region in str(cell_value):
                                        is_other_region = True
                                        break
                            if is_other_region:
                                break

                # íƒ€ì‹œë„ í•™êµë©´ í•´ë‹¹ í–‰ì˜ í•™êµëª… ì…€ë“¤ì„ ë¹¨ê°„ìƒ‰ìœ¼ë¡œ
                if is_other_region:
                    for col_name in ['í˜„ì¬ë³¸ì²­', 'ë°œë ¹ë³¸ì²­']:
                        if col_name in col_indices:
                            ws.cell(row=row_idx, column=col_indices[col_name]).fill = red_fill
                    colored_count += 1

            wb.save(output_path)

            if colored_count > 0:
                print(f"  ğŸ”´ íƒ€ì‹œë„ í•™êµ {colored_count}ê°œ ë¹¨ê°„ìƒ‰ í‘œì‹œ ì™„ë£Œ")

        except Exception as e:
            print(f"  âš ï¸  ì…€ ìƒ‰ìƒ í‘œì‹œ ì‹¤íŒ¨: {str(e)} (ë°ì´í„°ëŠ” ì •ìƒ ì €ì¥ë¨)")

        print(f"  âœ“ ì €ì¥ ì™„ë£Œ: {len(df)}í–‰, {len(df.columns)}ê°œ ì»¬ëŸ¼")

    def generate_report(self, output_path: str = None) -> str:
        """ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("=" * 60)
        report.append("ğŸ“‹ Excel Unifier ë¶„ì„ ë¦¬í¬íŠ¸")
        report.append("=" * 60)
        report.append(f"\nì´ íŒŒì¼ ìˆ˜: {len(self.dataframes)}")

        total_rows = sum(len(df_info['data']) for df_info in self.dataframes)
        report.append(f"ì´ í–‰ ìˆ˜: {total_rows}")

        report.append(f"\nì»¬ëŸ¼ ë§¤í•‘:")
        for unified_col, original_cols in self.column_mappings.items():
            if len(original_cols) > 1:
                report.append(f"  â€¢ {unified_col}")
                for orig in original_cols:
                    if orig != unified_col:
                        report.append(f"    - {orig}")

        report_text = "\n".join(report)

        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report_text)

        return report_text


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(
        description='ì—¬ëŸ¬ ì—‘ì…€ íŒŒì¼ì„ ë¶„ì„í•˜ê³  í†µì¼ëœ ì–‘ì‹ìœ¼ë¡œ í†µí•©í•©ë‹ˆë‹¤.'
    )
    parser.add_argument(
        'files',
        nargs='+',
        help='í†µí•©í•  ì—‘ì…€ íŒŒì¼ ê²½ë¡œë“¤'
    )
    parser.add_argument(
        '-o', '--output',
        default='unified_output.xlsx',
        help='ì¶œë ¥ íŒŒì¼ëª… (ê¸°ë³¸ê°’: unified_output.xlsx)'
    )
    parser.add_argument(
        '-k', '--key-columns',
        nargs='+',
        help='ì¤‘ë³µ íŒë‹¨ì— ì‚¬ìš©í•  í‚¤ ì»¬ëŸ¼ëª…ë“¤ (ì˜ˆ: ì´ë¦„ í•™êµ)'
    )
    parser.add_argument(
        '-t', '--threshold',
        type=int,
        default=85,
        help='ìœ ì‚¬ë„ ì„ê³„ê°’ 0-100 (ê¸°ë³¸ê°’: 85)'
    )
    parser.add_argument(
        '-r', '--report',
        help='ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥ ê²½ë¡œ'
    )
    parser.add_argument(
        '--ai',
        action='store_true',
        help='AI ê¸°ë°˜ ë§¤ì¹­ ì‚¬ìš© (Gemini API í•„ìš”)'
    )
    parser.add_argument(
        '--api-key',
        help='Gemini API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ GEMINI_API_KEY ì‚¬ìš©)'
    )

    args = parser.parse_args()

    # ExcelUnifier ì‹¤í–‰
    unifier = ExcelUnifier(
        similarity_threshold=args.threshold,
        use_ai=args.ai,
        gemini_api_key=args.api_key
    )
    unifier.load_excel_files(args.files)
    unifier.analyze_columns()

    # ë°ì´í„° í†µí•©
    unified_df = unifier.unify_dataframes(key_columns=args.key_columns)

    # ê²°ê³¼ ì €ì¥
    unifier.save_unified_excel(args.output, unified_df)

    # ë¦¬í¬íŠ¸ ìƒì„±
    report = unifier.generate_report(args.report)
    print("\n" + report)

    print(f"\nâœ¨ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {args.output}")


if __name__ == '__main__':
    main()
