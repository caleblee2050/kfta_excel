#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Excel Unifier - í†µì¼ë˜ì§€ ì•Šì€ ì—‘ì…€ íŒŒì¼ë“¤ì„ ë¶„ì„í•˜ê³  í†µí•©í•˜ëŠ” ë„êµ¬
"""

import pandas as pd
import os
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from fuzzywuzzy import fuzz
from collections import defaultdict
import json
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


class ExcelUnifier:
    def __init__(
        self,
        similarity_threshold: int = 85,
        use_ai: bool = False,
        gemini_api_key: Optional[str] = None
    ):
        """
        ì—‘ì…€ í†µí•©ê¸° ì´ˆê¸°í™”

        Args:
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (0-100, ê¸°ë³¸ê°’ 85)
            use_ai: AI ê¸°ë°˜ ë§¤ì¹­ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’ False)
            gemini_api_key: Gemini API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ìŒ)
        """
        self.similarity_threshold = similarity_threshold
        self.use_ai = use_ai
        self.dataframes = []
        self.column_mappings = {}
        self.unified_columns = []

        # AI ëª¨ë“œ ì´ˆê¸°í™”
        self.ai_matcher = None
        if use_ai:
            try:
                from ai_matcher import GeminiMatcher
                self.ai_matcher = GeminiMatcher(api_key=gemini_api_key)
                print("ğŸ¤– AI ëª¨ë“œ í™œì„±í™” (Gemini API)")
            except ImportError:
                print("âš ï¸  ai_matcher ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                self.use_ai = False
            except Exception as e:
                print(f"âš ï¸  AI ëª¨ë“œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}. ê¸°ë³¸ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                self.use_ai = False

    def load_excel_files(self, file_paths: List[str]) -> None:
        """ì—¬ëŸ¬ ì—‘ì…€ íŒŒì¼ ë¡œë“œ (ëª¨ë“  ì‹œíŠ¸ í¬í•¨)"""
        print(f"ğŸ“‚ {len(file_paths)}ê°œì˜ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤...")

        for file_path in file_paths:
            try:
                # ì—‘ì…€ íŒŒì¼ ì½ê¸° (.xlsx, .xls ëª¨ë‘ ì§€ì›)
                if file_path.endswith('.csv'):
                    df = pd.read_csv(file_path)
                    self.dataframes.append({
                        'path': file_path,
                        'sheet': None,
                        'data': df,
                        'columns': list(df.columns)
                    })
                    print(f"  âœ“ {os.path.basename(file_path)}: {len(df)}í–‰, {len(df.columns)}ê°œ ì»¬ëŸ¼")
                else:
                    # ì—‘ì…€ íŒŒì¼ì˜ ëª¨ë“  ì‹œíŠ¸ ì½ê¸°
                    excel_file = pd.ExcelFile(file_path)
                    sheet_names = excel_file.sheet_names

                    print(f"  ğŸ“„ {os.path.basename(file_path)}: {len(sheet_names)}ê°œ ì‹œíŠ¸ ë°œê²¬")

                    for sheet_name in sheet_names:
                        df = pd.read_excel(file_path, sheet_name=sheet_name)

                        # ë¹ˆ ì‹œíŠ¸ ê±´ë„ˆë›°ê¸°
                        if df.empty or len(df.columns) == 0:
                            print(f"    âŠ˜ ì‹œíŠ¸ '{sheet_name}': ë¹ˆ ì‹œíŠ¸ (ê±´ë„ˆëœ€)")
                            continue

                        self.dataframes.append({
                            'path': file_path,
                            'sheet': sheet_name,
                            'data': df,
                            'columns': list(df.columns)
                        })
                        print(f"    âœ“ ì‹œíŠ¸ '{sheet_name}': {len(df)}í–‰, {len(df.columns)}ê°œ ì»¬ëŸ¼")

            except Exception as e:
                print(f"  âœ— {file_path} ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

    def analyze_columns(self) -> Dict[str, List[str]]:
        """
        ëª¨ë“  íŒŒì¼ì˜ ì»¬ëŸ¼ëª…ì„ ë¶„ì„í•˜ê³  ìœ ì‚¬í•œ ì»¬ëŸ¼ë¼ë¦¬ ê·¸ë£¹í™”

        Returns:
            ì»¬ëŸ¼ ê·¸ë£¹ ë”•ì…”ë„ˆë¦¬ {í†µí•©ì»¬ëŸ¼ëª…: [ì›ë³¸ì»¬ëŸ¼ëª…ë“¤]}
        """
        print("\nğŸ” ì»¬ëŸ¼ëª… ë¶„ì„ ì¤‘...")

        # í‚¤ì›Œë“œ ê¸°ë°˜ ì»¬ëŸ¼ ë§¤í•‘ ê·œì¹™
        keyword_mappings = {
            'ì´ë¦„': ['ì´ë¦„', 'ì„±ëª…', 'í•™ìƒëª…', 'ì´ë¦„', 'ì„±í•¨', 'ì´ ë¦„'],
            'í•™êµ': ['í•™êµ', 'ëŒ€í•™êµ', 'ì†Œì†ëŒ€í•™', 'ëŒ€í•™', 'í•™ êµ', 'ì†Œì†í•™êµ'],
            'ì „ê³µ': ['ì „ê³µ', 'ì „ê³µë¶„ì•¼', 'ì „ê³µê³¼ëª©', 'í•™ê³¼', 'ì „ ê³µ'],
            'í•™ë…„': ['í•™ë…„', 'í•™ ë…„', 'ì—°ì°¨'],
            'ì—°ë½ì²˜': ['ì—°ë½ì²˜', 'ì „í™”ë²ˆí˜¸', 'íœ´ëŒ€í°', 'ì „í™”', 'ì—° ë½ ì²˜', 'í•¸ë“œí°', 'HP', 'íœ´ëŒ€ì „í™”'],
            'ì´ë©”ì¼': ['ì´ë©”ì¼', 'ë©”ì¼', 'email', 'e-mail', 'ì´ ë©” ì¼'],
            'ì£¼ì†Œ': ['ì£¼ì†Œ', 'ì£¼ ì†Œ', 'ê±°ì£¼ì§€', 'ì§‘ì£¼ì†Œ'],
        }

        # ëª¨ë“  ì»¬ëŸ¼ëª… ìˆ˜ì§‘
        all_columns = []
        for df_info in self.dataframes:
            all_columns.extend(df_info['columns'])

        # ì¤‘ë³µ ì œê±°í•˜ë˜ ë¹ˆë„ìˆ˜ ìœ ì§€
        column_freq = defaultdict(int)
        for col in all_columns:
            column_freq[col] += 1

        unique_columns = list(column_freq.keys())

        # ì»¬ëŸ¼ ê·¸ë£¹í™”
        column_groups = {}
        processed = set()

        # ë¨¼ì € í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤í•‘ ì ìš©
        for unified_name, keywords in keyword_mappings.items():
            matched_cols = []
            for col in unique_columns:
                if col in processed:
                    continue
                col_normalized = col.strip().replace(' ', '').lower()
                for keyword in keywords:
                    keyword_normalized = keyword.strip().replace(' ', '').lower()
                    # ì •í™•íˆ ì¼ì¹˜í•˜ê±°ë‚˜ í¬í•¨í•˜ëŠ” ê²½ìš°
                    if col_normalized == keyword_normalized or keyword_normalized in col_normalized:
                        matched_cols.append(col)
                        processed.add(col)
                        break

            if matched_cols:
                column_groups[unified_name] = matched_cols
                if len(matched_cols) > 1:
                    print(f"  ğŸ“Œ '{unified_name}' â† {matched_cols}")

        # ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ë“¤ì€ ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ë§¤í•‘
        for i, col1 in enumerate(unique_columns):
            if col1 in processed:
                continue

            # í˜„ì¬ ì»¬ëŸ¼ê³¼ ìœ ì‚¬í•œ ì»¬ëŸ¼ë“¤ ì°¾ê¸°
            similar_cols = [col1]
            processed.add(col1)

            for col2 in unique_columns[i+1:]:
                if col2 in processed:
                    continue

                # ìœ ì‚¬ë„ ê³„ì‚° (AI ëª¨ë“œ ë˜ëŠ” ê¸°ë³¸ ëª¨ë“œ)
                is_similar = False

                if self.use_ai and self.ai_matcher:
                    # AI ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
                    try:
                        result = self.ai_matcher.calculate_semantic_similarity(
                            col1, col2,
                            context="ì—‘ì…€ ì»¬ëŸ¼ëª…"
                        )
                        is_similar = result['is_similar']
                        if is_similar:
                            print(f"  ğŸ¤– AI ë§¤ì¹­: '{col1}' â†” '{col2}' ({result['similarity']}%, {result['reason']})")
                    except Exception as e:
                        print(f"  âš ï¸  AI ë¶„ì„ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë“œë¡œ ì „í™˜: {str(e)}")
                        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ëª¨ë“œë¡œ fallback
                        similarity = fuzz.ratio(col1, col2)
                        is_similar = similarity >= self.similarity_threshold
                else:
                    # ê¸°ë³¸ ëª¨ë“œ: Levenshtein Distance
                    similarity = fuzz.ratio(col1, col2)
                    is_similar = similarity >= self.similarity_threshold

                if is_similar:
                    similar_cols.append(col2)
                    processed.add(col2)

            # ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ ì»¬ëŸ¼ëª…ì„ ëŒ€í‘œ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì„ íƒ
            representative = max(similar_cols, key=lambda x: column_freq[x])
            column_groups[representative] = similar_cols

            if len(similar_cols) > 1:
                print(f"  ğŸ“Œ '{representative}' â† {similar_cols}")

        self.column_mappings = column_groups
        self.unified_columns = list(column_groups.keys())

        return column_groups

    def normalize_value(self, value: str, value_type: str = 'general') -> str:
        """
        ê°’ ì •ê·œí™”

        Args:
            value: ì •ê·œí™”í•  ê°’
            value_type: ê°’ì˜ íƒ€ì… (general, school, name ë“±)
        """
        if pd.isna(value):
            return ""

        value = str(value).strip()

        # í•™êµëª… ì •ê·œí™”
        if value_type == 'school':
            # ëŒ€í•™êµ, ëŒ€í•™, ê³ ë“±í•™êµ, ì¤‘í•™êµ ë“± í†µì¼
            replacements = [
                ('å¤§å­¸æ ¡', ''),
                ('å¤§å­¦æ ¡', ''),
                ('å¤§å­¦', ''),
                ('ëŒ€í•™êµ', ''),
                ('ëŒ€í•™', ''),
                ('ê³ ë“±í•™êµ', ''),
                ('ê³ êµ', ''),
                ('ì¤‘í•™êµ', ''),
                ('ì¤‘í•™', ''),
                (' ', ''),
            ]
            for old, new in replacements:
                value = value.replace(old, new)

        return value.lower()

    def find_similar_values(self, values: List[str], threshold: int = None) -> Dict[str, List[str]]:
        """
        ìœ ì‚¬í•œ ê°’ë“¤ì„ ê·¸ë£¹í™”

        Args:
            values: ê°’ ë¦¬ìŠ¤íŠ¸
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’

        Returns:
            {ëŒ€í‘œê°’: [ìœ ì‚¬í•œ ê°’ë“¤]} ë”•ì…”ë„ˆë¦¬
        """
        if threshold is None:
            threshold = self.similarity_threshold

        unique_values = list(set([str(v) for v in values if pd.notna(v) and str(v).strip()]))

        value_groups = {}
        processed = set()

        for i, val1 in enumerate(unique_values):
            if val1 in processed:
                continue

            similar_vals = [val1]
            processed.add(val1)

            for val2 in unique_values[i+1:]:
                if val2 in processed:
                    continue

                similarity = fuzz.ratio(val1, val2)
                if similarity >= threshold:
                    similar_vals.append(val2)
                    processed.add(val2)

            # ê°€ì¥ ê¸´ ê°’ì„ ëŒ€í‘œê°’ìœ¼ë¡œ (ë³´í†µ ë” ì™„ì „í•œ í˜•íƒœ)
            representative = max(similar_vals, key=len)
            value_groups[representative] = similar_vals

        return value_groups

    def unify_dataframes(self, key_columns: List[str] = None) -> pd.DataFrame:
        """
        ëª¨ë“  ë°ì´í„°í”„ë ˆì„ì„ í†µí•©

        Args:
            key_columns: ì¤‘ë³µ íŒë‹¨ì— ì‚¬ìš©í•  í‚¤ ì»¬ëŸ¼ë“¤ (ì˜ˆ: ['ì´ë¦„', 'í•™êµ'])

        Returns:
            í†µí•©ëœ ë°ì´í„°í”„ë ˆì„
        """
        print("\nğŸ”„ ë°ì´í„° í†µí•© ì¤‘...")

        if not self.column_mappings:
            self.analyze_columns()

        # í†µí•© ë°ì´í„°í”„ë ˆì„ ì´ˆê¸°í™”
        unified_data = []

        # ê° íŒŒì¼ì˜ ë°ì´í„°ë¥¼ í†µì¼ëœ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë³€í™˜
        for df_info in self.dataframes:
            df = df_info['data'].copy()

            # ì»¬ëŸ¼ëª… ë§¤í•‘
            rename_dict = {}
            for unified_col, original_cols in self.column_mappings.items():
                for orig_col in original_cols:
                    if orig_col in df.columns:
                        rename_dict[orig_col] = unified_col

            df_renamed = df.rename(columns=rename_dict)

            # ì¤‘ë³µëœ ì»¬ëŸ¼ëª… ì²˜ë¦¬ (ê°™ì€ ì´ë¦„ì˜ ì»¬ëŸ¼ì´ ì—¬ëŸ¬ ê°œ ìˆëŠ” ê²½ìš°)
            if df_renamed.columns.duplicated().any():
                # ì¤‘ë³µ ì»¬ëŸ¼ ì œê±° (ì²« ë²ˆì§¸ë§Œ ìœ ì§€)
                df_renamed = df_renamed.loc[:, ~df_renamed.columns.duplicated()]

            # ëˆ„ë½ëœ ì»¬ëŸ¼ ì¶”ê°€ (ë¹ˆ ê°’ìœ¼ë¡œ)
            for col in self.unified_columns:
                if col not in df_renamed.columns:
                    df_renamed[col] = ""

            # í†µì¼ëœ ì»¬ëŸ¼ë§Œ ì„ íƒ
            df_unified = df_renamed[self.unified_columns]
            unified_data.append(df_unified)

            # ì‹œíŠ¸ ì •ë³´ í¬í•¨í•˜ì—¬ ì¶œë ¥
            file_name = os.path.basename(df_info['path'])
            sheet_info = f" (ì‹œíŠ¸: {df_info['sheet']})" if df_info.get('sheet') else ""
            print(f"  âœ“ {file_name}{sheet_info}: {len(df_unified)}í–‰ ë³€í™˜")

        # ëª¨ë“  ë°ì´í„° ê²°í•©
        result_df = pd.concat(unified_data, ignore_index=True)
        print(f"\nğŸ“Š í†µí•© ì™„ë£Œ: ì´ {len(result_df)}í–‰")

        # í‚¤ ì»¬ëŸ¼ì´ ì§€ì •ëœ ê²½ìš° ì¤‘ë³µ ì œê±°
        if key_columns:
            print(f"\nğŸ”‘ í‚¤ ì»¬ëŸ¼ {key_columns}ë¡œ ì¤‘ë³µ í™•ì¸ ì¤‘...")

            # í‚¤ ì»¬ëŸ¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            valid_keys = [k for k in key_columns if k in result_df.columns]

            if valid_keys:
                before_count = len(result_df)
                result_df = self._remove_duplicates_smart(result_df, valid_keys)
                after_count = len(result_df)
                removed = before_count - after_count

                if removed > 0:
                    print(f"  âœ“ {removed}ê°œì˜ ì¤‘ë³µ í–‰ ì œê±°ë¨")
                else:
                    print(f"  â„¹ ì¤‘ë³µ í–‰ì´ ë°œê²¬ë˜ì§€ ì•ŠìŒ")

        return result_df

    def _remove_duplicates_smart(self, df: pd.DataFrame, key_columns: List[str]) -> pd.DataFrame:
        """
        ìŠ¤ë§ˆíŠ¸ ì¤‘ë³µ ì œê±° - ìœ ì‚¬í•œ ê°’ë„ ê°™ì€ ê²ƒìœ¼ë¡œ ê°„ì£¼
        """
        # ì •ê·œí™”ëœ í‚¤ ì»¬ëŸ¼ ìƒì„±
        normalized_df = df.copy()

        for col in key_columns:
            if col in df.columns:
                # ê°’ ì •ê·œí™”
                col_type = 'school' if 'í•™êµ' in col else 'general'
                normalized_df[f'{col}_normalized'] = df[col].apply(
                    lambda x: self.normalize_value(x, col_type)
                )

        # ì •ê·œí™”ëœ ê°’ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
        normalized_keys = [f'{col}_normalized' for col in key_columns if col in df.columns]

        if normalized_keys:
            # ì¤‘ë³µ ì¤‘ ì²« ë²ˆì§¸ í–‰ ìœ ì§€ (ê°€ì¥ ì™„ì „í•œ ë°ì´í„°ë¥¼ ê°€ì§„ í–‰ ì„ íƒ)
            result_df = normalized_df.drop_duplicates(subset=normalized_keys, keep='first')

            # ì •ê·œí™” ì»¬ëŸ¼ ì œê±°
            result_df = result_df.drop(columns=normalized_keys)
        else:
            result_df = df

        return result_df

    def save_unified_excel(self, output_path: str, df: pd.DataFrame = None) -> None:
        """í†µí•©ëœ ë°ì´í„°ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥"""
        if df is None:
            df = self.unify_dataframes()

        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘: {output_path}")

        # ì—‘ì…€ë¡œ ì €ì¥
        df.to_excel(output_path, index=False, engine='openpyxl')

        print(f"  âœ“ ì €ì¥ ì™„ë£Œ: {len(df)}í–‰, {len(df.columns)}ê°œ ì»¬ëŸ¼")

    def generate_report(self, output_path: str = None) -> str:
        """ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("=" * 60)
        report.append("ğŸ“‹ Excel Unifier ë¶„ì„ ë¦¬í¬íŠ¸")
        report.append("=" * 60)
        report.append(f"\nì´ íŒŒì¼ ìˆ˜: {len(self.dataframes)}")

        total_rows = sum(len(df_info['data']) for df_info in self.dataframes)
        report.append(f"ì´ í–‰ ìˆ˜: {total_rows}")

        report.append(f"\nì»¬ëŸ¼ ë§¤í•‘:")
        for unified_col, original_cols in self.column_mappings.items():
            if len(original_cols) > 1:
                report.append(f"  â€¢ {unified_col}")
                for orig in original_cols:
                    if orig != unified_col:
                        report.append(f"    - {orig}")

        report_text = "\n".join(report)

        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report_text)

        return report_text


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(
        description='ì—¬ëŸ¬ ì—‘ì…€ íŒŒì¼ì„ ë¶„ì„í•˜ê³  í†µì¼ëœ ì–‘ì‹ìœ¼ë¡œ í†µí•©í•©ë‹ˆë‹¤.'
    )
    parser.add_argument(
        'files',
        nargs='+',
        help='í†µí•©í•  ì—‘ì…€ íŒŒì¼ ê²½ë¡œë“¤'
    )
    parser.add_argument(
        '-o', '--output',
        default='unified_output.xlsx',
        help='ì¶œë ¥ íŒŒì¼ëª… (ê¸°ë³¸ê°’: unified_output.xlsx)'
    )
    parser.add_argument(
        '-k', '--key-columns',
        nargs='+',
        help='ì¤‘ë³µ íŒë‹¨ì— ì‚¬ìš©í•  í‚¤ ì»¬ëŸ¼ëª…ë“¤ (ì˜ˆ: ì´ë¦„ í•™êµ)'
    )
    parser.add_argument(
        '-t', '--threshold',
        type=int,
        default=85,
        help='ìœ ì‚¬ë„ ì„ê³„ê°’ 0-100 (ê¸°ë³¸ê°’: 85)'
    )
    parser.add_argument(
        '-r', '--report',
        help='ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥ ê²½ë¡œ'
    )
    parser.add_argument(
        '--ai',
        action='store_true',
        help='AI ê¸°ë°˜ ë§¤ì¹­ ì‚¬ìš© (Gemini API í•„ìš”)'
    )
    parser.add_argument(
        '--api-key',
        help='Gemini API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ GEMINI_API_KEY ì‚¬ìš©)'
    )

    args = parser.parse_args()

    # ExcelUnifier ì‹¤í–‰
    unifier = ExcelUnifier(
        similarity_threshold=args.threshold,
        use_ai=args.ai,
        gemini_api_key=args.api_key
    )
    unifier.load_excel_files(args.files)
    unifier.analyze_columns()

    # ë°ì´í„° í†µí•©
    unified_df = unifier.unify_dataframes(key_columns=args.key_columns)

    # ê²°ê³¼ ì €ì¥
    unifier.save_unified_excel(args.output, unified_df)

    # ë¦¬í¬íŠ¸ ìƒì„±
    report = unifier.generate_report(args.report)
    print("\n" + report)

    print(f"\nâœ¨ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {args.output}")


if __name__ == '__main__':
    main()
