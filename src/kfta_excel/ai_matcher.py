#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI ê¸°ë°˜ ìœ ì‚¬ë„ ë§¤ì¹­ ëª¨ë“ˆ
Gemini APIë¥¼ í™œìš©í•œ ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„ ë¶„ì„
"""

import json
import os
import time
from functools import lru_cache
from typing import Dict, List, Optional

try:
    # New SDK (preferred)
    from google import genai as genai_sdk
except ImportError:  # pragma: no cover - optional dependency fallback
    genai_sdk = None

try:
    # Legacy SDK fallback
    import google.generativeai as legacy_genai
except ImportError:  # pragma: no cover - optional dependency fallback
    legacy_genai = None


class GeminiMatcher:
    """Gemini AIë¥¼ í™œìš©í•œ ìŠ¤ë§ˆíŠ¸ ë§¤ì¹­"""

    DEFAULT_MODEL_CANDIDATES = (
        "gemini-3-flash",
        "gemini-2.5-flash",
        "gemini-2.0-flash",
    )

    def __init__(
        self,
        api_key: Optional[str] = None,
        model_name: Optional[str] = None,
        fallback_models: Optional[List[str]] = None,
    ):
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            raise ValueError(
                "Gemini API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. "
                "GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ api_key íŒŒë¼ë¯¸í„°ë¥¼ ì „ë‹¬í•˜ì„¸ìš”."
            )

        self.model_candidates = self._build_model_candidates(model_name, fallback_models)
        self.active_model_name = self.model_candidates[0]
        self.backend = self._init_backend()
        print(f"ğŸ¤– Gemini ëª¨ë¸ í™œì„±í™”: {self.active_model_name} ({self.backend})")

        # ê°™ì€ ìš”ì²­ ë°˜ë³µ í˜¸ì¶œ ë°©ì§€
        self.cache = {}

    def _init_backend(self) -> str:
        if genai_sdk is not None:
            self.client = genai_sdk.Client(api_key=self.api_key)
            return "google.genai"

        if legacy_genai is not None:
            legacy_genai.configure(api_key=self.api_key)
            self.client = legacy_genai.GenerativeModel(self.active_model_name)
            return "google.generativeai"

        raise ImportError(
            "Gemini SDKë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
            "google-genai ë˜ëŠ” google-generativeai íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”."
        )

    @classmethod
    def _build_model_candidates(
        cls,
        model_name: Optional[str],
        fallback_models: Optional[List[str]],
    ) -> List[str]:
        env_model = os.getenv("GEMINI_MODEL", "").strip()
        primary = (model_name or env_model or cls.DEFAULT_MODEL_CANDIDATES[0]).strip()

        candidates = [primary]
        if fallback_models:
            candidates.extend([m.strip() for m in fallback_models if m and m.strip()])
        candidates.extend(cls.DEFAULT_MODEL_CANDIDATES)
        if env_model:
            candidates.append(env_model)

        deduped = []
        seen = set()
        for name in candidates:
            if name and name not in seen:
                deduped.append(name)
                seen.add(name)
        return deduped

    @staticmethod
    def _is_retryable_model_error(error: Exception) -> bool:
        message = str(error).lower()
        retryable_keywords = (
            "404",
            "not found",
            "unsupported",
            "invalid model",
            "resource has been exhausted",
            "quota",
            "permission denied",
        )
        return any(keyword in message for keyword in retryable_keywords)

    def _switch_model(self, model_name: str) -> None:
        self.active_model_name = model_name
        if self.backend == "google.generativeai":
            self.client = legacy_genai.GenerativeModel(model_name)
        print(f"ğŸ” Gemini ëª¨ë¸ ì „í™˜: {model_name}")

    @staticmethod
    def _extract_text_from_new_sdk_response(response) -> str:
        text = getattr(response, "text", None)
        if text:
            return text

        candidates = getattr(response, "candidates", None) or []
        for candidate in candidates:
            content = getattr(candidate, "content", None)
            if not content:
                continue
            parts = getattr(content, "parts", None) or []
            chunks = [getattr(part, "text", "") for part in parts if getattr(part, "text", "")]
            if chunks:
                return "\n".join(chunks)

        return str(response)

    def _generate_content(self, prompt: str) -> str:
        last_error = None
        tried = set()
        ordered_candidates = [self.active_model_name] + self.model_candidates

        for model_name in ordered_candidates:
            if model_name in tried:
                continue
            tried.add(model_name)

            try:
                if model_name != self.active_model_name:
                    self._switch_model(model_name)

                if self.backend == "google.genai":
                    response = self.client.models.generate_content(
                        model=self.active_model_name,
                        contents=prompt,
                    )
                    return self._extract_text_from_new_sdk_response(response)

                response = self.client.generate_content(prompt)
                return response.text
            except Exception as error:
                last_error = error
                if not self._is_retryable_model_error(error):
                    break
                print(f"âš ï¸ ëª¨ë¸ '{model_name}' í˜¸ì¶œ ì‹¤íŒ¨, ë‹¤ìŒ ëª¨ë¸ë¡œ í´ë°±: {error}")

        tried_models = ", ".join(ordered_candidates)
        raise RuntimeError(
            f"Gemini ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„: {tried_models}): {last_error}"
        ) from last_error

    @staticmethod
    def _strip_json_block(text: str) -> str:
        result_text = text.strip()
        if result_text.startswith("```json"):
            result_text = result_text[7:]
        if result_text.endswith("```"):
            result_text = result_text[:-3]
        return result_text.strip()

    @lru_cache(maxsize=1000)
    def calculate_semantic_similarity(
        self,
        text1: str,
        text2: str,
        context: str = "ì—‘ì…€ ì»¬ëŸ¼ëª…",
    ) -> Dict[str, any]:
        cache_key = f"{text1}||{text2}||{context}"
        if cache_key in self.cache:
            return self.cache[cache_key]

        prompt = f"""
ë‹¹ì‹ ì€ ì—‘ì…€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‘ í…ìŠ¤íŠ¸ê°€ ê°™ì€ ì˜ë¯¸ì¸ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.

**ì»¨í…ìŠ¤íŠ¸**: {context}

**í…ìŠ¤íŠ¸ 1**: "{text1}"
**í…ìŠ¤íŠ¸ 2**: "{text2}"

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”:
1. ë™ì¼í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ”ê°€? (ì˜ˆ: "ì´ë¦„"ê³¼ "ì„±ëª…", "í•™êµ"ì™€ "ëŒ€í•™êµ")
2. ìœ ì‚¬í•œ ê°œë…ì¸ê°€? (ì˜ˆ: "ì „ê³µ"ê³¼ "ì „ê³µë¶„ì•¼")
3. ë‹¤êµ­ì–´ í‘œí˜„ì¸ê°€? (ì˜ˆ: "name"ê³¼ "ì´ë¦„")
4. ì•½ì–´ì™€ ì „ì²´ í‘œí˜„ì¸ê°€? (ì˜ˆ: "HP"ì™€ "íœ´ëŒ€í°", "email"ê³¼ "ì´ë©”ì¼")
5. í•œìì™€ í•œê¸€ í‘œí˜„ì¸ê°€? (ì˜ˆ: "å¤§å­¸"ê³¼ "ëŒ€í•™")

**ì‘ë‹µ í˜•ì‹** (JSON):
{{
    "similarity": 0-100 ì‚¬ì´ì˜ ìˆ«ì (100ì´ ì™„ì „ ë™ì¼),
    "is_similar": true ë˜ëŠ” false (ìœ ì‚¬ë„ 70 ì´ìƒì´ë©´ true),
    "reason": "íŒë‹¨ ì´ìœ ë¥¼ í•œêµ­ì–´ë¡œ ì„¤ëª…",
    "mapping": "í†µì¼ëœ í‘œí˜„ ì œì•ˆ"
}}

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤.
"""
        try:
            raw = self._generate_content(prompt)
            result = json.loads(self._strip_json_block(raw))
            self.cache[cache_key] = result
            return result
        except Exception as error:
            print(f"âš ï¸  AI ë¶„ì„ ì‹¤íŒ¨ ({text1} â†” {text2}): {error}")
            return {
                "similarity": 0,
                "is_similar": False,
                "reason": f"AI ë¶„ì„ ì‹¤íŒ¨: {error}",
                "mapping": text1,
            }

    def match_columns_batch(self, columns_list: List[List[str]]) -> Dict[str, List[str]]:
        print("\nğŸ¤– AI ê¸°ë°˜ ì»¬ëŸ¼ ë§¤ì¹­ ì‹œì‘...")

        all_columns = []
        for columns in columns_list:
            all_columns.extend(columns)
        unique_columns = list(set(all_columns))

        column_groups = {}
        processed = set()

        for i, col1 in enumerate(unique_columns):
            if col1 in processed:
                continue

            group = [col1]
            processed.add(col1)

            for col2 in unique_columns[i + 1:]:
                if col2 in processed:
                    continue

                result = self.calculate_semantic_similarity(
                    col1,
                    col2,
                    context="ì—‘ì…€ ì»¬ëŸ¼ëª… (í•™ìƒ/êµì‚¬ ì •ë³´)",
                )
                if result["is_similar"]:
                    group.append(col2)
                    processed.add(col2)
                    print(
                        f"  ğŸ”— ë§¤ì¹­: '{col1}' â†” '{col2}' "
                        f"(ìœ ì‚¬ë„: {result['similarity']}%, {result['reason']})"
                    )
                time.sleep(0.1)

            representative = self._select_best_column_name(group) if len(group) > 1 else col1
            column_groups[representative] = group

        return column_groups

    def _select_best_column_name(self, column_names: List[str]) -> str:
        if len(column_names) == 1:
            return column_names[0]

        prompt = f"""
ë‹¤ìŒ ì»¬ëŸ¼ëª…ë“¤ ì¤‘ ê°€ì¥ í‘œì¤€ì ì´ê³  ëª…í™•í•œ ê²ƒì„ í•˜ë‚˜ ì„ íƒí•˜ì„¸ìš”:

{', '.join([f'"{name}"' for name in column_names])}

ì„ íƒ ê¸°ì¤€:
1. ê°€ì¥ ëª…í™•í•˜ê³  í‘œì¤€ì ì¸ í‘œí˜„
2. í•œê¸€ > ì˜ì–´ (í•œêµ­ ë°ì´í„°ì´ë¯€ë¡œ)
3. ì „ì²´ í‘œí˜„ > ì•½ì–´
4. ì¼ë°˜ì ìœ¼ë¡œ ë§ì´ ì‚¬ìš©ë˜ëŠ” í‘œí˜„

**ì‘ë‹µ í˜•ì‹**: ì„ íƒí•œ ì»¬ëŸ¼ëª…ë§Œ ì¶œë ¥ (ë”°ì˜´í‘œ ì—†ì´)
"""
        try:
            selected = self._generate_content(prompt).strip().strip("\"'")
            return selected if selected in column_names else column_names[0]
        except Exception as error:
            print(f"âš ï¸  ì»¬ëŸ¼ëª… ì„ íƒ ì‹¤íŒ¨: {error}")
            return column_names[0]

    def match_values_smart(
        self,
        values: List[str],
        value_type: str = "ì¼ë°˜",
    ) -> Dict[str, List[str]]:
        unique_values = list(set([str(v) for v in values if v and str(v).strip()]))
        if len(unique_values) <= 1:
            return {unique_values[0]: unique_values} if unique_values else {}

        print(f"\nğŸ¤– AI ê¸°ë°˜ ê°’ ë§¤ì¹­ ì‹œì‘ (íƒ€ì…: {value_type})...")

        value_groups = {}
        processed = set()

        for i, val1 in enumerate(unique_values):
            if val1 in processed:
                continue

            group = [val1]
            processed.add(val1)
            representative_suggestion = None

            for val2 in unique_values[i + 1:]:
                if val2 in processed:
                    continue

                result = self.calculate_semantic_similarity(
                    val1,
                    val2,
                    context=f"{value_type} ê°’",
                )

                if result["is_similar"]:
                    group.append(val2)
                    processed.add(val2)
                    if result.get("mapping"):
                        representative_suggestion = result["mapping"]
                    print(f"  ğŸ”— ë§¤ì¹­: '{val1}' â†” '{val2}' (ìœ ì‚¬ë„: {result['similarity']}%)")

                time.sleep(0.1)

            representative = representative_suggestion if len(group) > 1 and representative_suggestion else max(group, key=len)
            value_groups[representative] = group

        return value_groups

    def verify_and_expand_school_name(
        self,
        school_name: str,
        gangwon_regions: Dict[str, str],
    ) -> Dict[str, str]:
        if not school_name or not school_name.strip():
            return {
                "full_name": school_name,
                "education_office": "",
                "region": "",
                "confidence": 0,
                "explanation": "ë¹ˆ í•™êµëª…",
            }

        cache_key = f"school_verify||{school_name}"
        if cache_key in self.cache:
            return self.cache[cache_key]

        region_list = ", ".join(gangwon_regions.keys())
        prompt = f"""
ë‹¹ì‹ ì€ ê°•ì›ë„ êµìœ¡ì²­ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•™êµëª…ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.

**ì…ë ¥ í•™êµëª…**: "{school_name}"
**ê°•ì›ë„ ì§€ì—­ ëª©ë¡**: {region_list}

ì‘ì—…:
1. ì•½ì¹­ì´ë©´ ì •ì‹ ëª…ì¹­ìœ¼ë¡œ í™•ì¥
2. ê°•ì›ë„ ë‚´ í•™êµ ì—¬ë¶€ì™€ ì§€ì—­ íŒë³„
3. confidence 0-100 ì‚°ì •

ì‘ë‹µ í˜•ì‹(JSON):
{{
    "full_name": "ì •ì‹ í•™êµëª…",
    "region": "ì§€ì—­ëª…",
    "confidence": 0,
    "explanation": "íŒë‹¨ ê·¼ê±°"
}}
"""
        try:
            raw = self._generate_content(prompt)
            result = json.loads(self._strip_json_block(raw))
            region = result.get("region", "")
            result["education_office"] = gangwon_regions.get(region, "") if region else ""
            self.cache[cache_key] = result
            return result
        except Exception as error:
            print(f"âš ï¸  í•™êµëª… ê²€ì¦ ì‹¤íŒ¨ ({school_name}): {error}")
            return {
                "full_name": school_name,
                "education_office": "",
                "region": "",
                "confidence": 0,
                "explanation": f"AI ë¶„ì„ ì‹¤íŒ¨: {error}",
            }


def test_gemini_matcher():
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        print("âŒ GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”")
        return

    matcher = GeminiMatcher(api_key)
    test_pairs = [("ì´ë¦„", "ì„±ëª…"), ("í•™êµ", "ëŒ€í•™êµ"), ("ì „ê³µ", "ì „ê³µë¶„ì•¼")]
    for col1, col2 in test_pairs:
        result = matcher.calculate_semantic_similarity(col1, col2)
        print(col1, col2, result)


if __name__ == "__main__":
    test_gemini_matcher()
